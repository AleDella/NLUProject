{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLU.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AleDella/NLUProject/blob/main/NLU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8tVQpX1RRyU"
      },
      "source": [
        "# **Dataset Functions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-470-dUjbTgP",
        "outputId": "04112c7b-9d4a-4779-a42e-eff9ebada961"
      },
      "source": [
        "# Download and unzip the dataset\n",
        "!wget https://data.deepai.org/ptbdataset.zip\n",
        "!unzip ptbdataset.zip -d data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-29 17:11:53--  https://data.deepai.org/ptbdataset.zip\n",
            "Resolving data.deepai.org (data.deepai.org)... 138.201.36.183\n",
            "Connecting to data.deepai.org (data.deepai.org)|138.201.36.183|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4568253 (4.4M) [application/x-zip-compressed]\n",
            "Saving to: ‘ptbdataset.zip’\n",
            "\n",
            "ptbdataset.zip      100%[===================>]   4.36M  6.88MB/s    in 0.6s    \n",
            "\n",
            "2021-05-29 17:11:54 (6.88 MB/s) - ‘ptbdataset.zip’ saved [4568253/4568253]\n",
            "\n",
            "Archive:  ptbdataset.zip\n",
            "  inflating: data/README             \n",
            "  inflating: data/ptb.char.test.txt  \n",
            "  inflating: data/ptb.char.train.txt  \n",
            "  inflating: data/ptb.char.valid.txt  \n",
            "  inflating: data/ptb.test.txt       \n",
            "  inflating: data/ptb.train.txt      \n",
            "  inflating: data/ptb.valid.txt      \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBglsunSbdtu",
        "outputId": "2e6a1d40-8752-4d86-ed86-e8792298ff93"
      },
      "source": [
        "# The file imported here is an external file written by TensorFlow authors\n",
        "# it is subject to Apache Licence Version 2.0\n",
        "# In order to check the whole file go here https://github.com/deeplearningathome/pytorch-language-model/blob/5a0f888560ec6adfb366080f8f874f18b06caf14/reader.py\n",
        "# NOTE: in the cell after this there is the whole reader.py file updated for the current torch version.\n",
        "!mkdir data\n",
        "!wget -q -O data/ptb.zip https://ibm.box.com/shared/static/z2yvmhbskc45xd2a9a4kkn6hg4g4kj5r.zip\n",
        "!unzip -o data/ptb.zip -d data\n",
        "!cp data/ptb/reader.py ."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘data’: File exists\n",
            "Archive:  data/ptb.zip\n",
            "   creating: data/ptb/\n",
            "  inflating: data/ptb/reader.py      \n",
            "   creating: data/__MACOSX/\n",
            "   creating: data/__MACOSX/ptb/\n",
            "  inflating: data/__MACOSX/ptb/._reader.py  \n",
            "  inflating: data/__MACOSX/._ptb     \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMZJzITHcpym"
      },
      "source": [
        "# Reader.py\n",
        "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ==============================================================================\n",
        "\"\"\"Utilities for parsing PTB text files.\"\"\"\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import collections\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Read the whole set and replaces the \"\\n\" with \"<eos>\" tag\n",
        "def _read_words(filename):\n",
        "  with tf.io.gfile.GFile(filename, \"r\") as f:\n",
        "    return f.read().replace(\"\\n\", \"<eos>\").split()\n",
        "\n",
        "# Builds the dictionaries for the words in the dataset\n",
        "# Associate ad each word an index\n",
        "def _build_vocab(filename):\n",
        "  data = _read_words(filename)\n",
        "\n",
        "  counter = collections.Counter(data)\n",
        "  count_pairs = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n",
        "\n",
        "  words, _ = list(zip(*count_pairs))\n",
        "  word_to_id = dict(zip(words, range(len(words))))\n",
        "  id_to_word = dict((v, k) for k, v in word_to_id.items())\n",
        "\n",
        "  return word_to_id, id_to_word\n",
        "\n",
        "# Convert a whole file into indeces\n",
        "def _file_to_word_ids(filename, word_to_id):\n",
        "  data = _read_words(filename)\n",
        "  return [word_to_id[word] for word in data if word in word_to_id]\n",
        "\n",
        "# Get the raw PTB dataset data\n",
        "def ptb_raw_data(data_path=None, prefix=\"ptb\"):\n",
        "  \"\"\"Load PTB raw data from data directory \"data_path\".\n",
        "  Reads PTB text files, converts strings to integer ids,\n",
        "  and performs mini-batching of the inputs.\n",
        "  The PTB dataset comes from Tomas Mikolov's webpage:\n",
        "  http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz\n",
        "  Args:\n",
        "    data_path: string path to the directory where simple-examples.tgz has\n",
        "      been extracted.\n",
        "  Returns:\n",
        "    tuple (train_data, valid_data, test_data, vocabulary)\n",
        "    where each of the data objects can be passed to PTBIterator.\n",
        "  \"\"\"\n",
        "\n",
        "  train_path = os.path.join(data_path, prefix + \".train.txt\")\n",
        "  valid_path = os.path.join(data_path, prefix + \".valid.txt\")\n",
        "  test_path = os.path.join(data_path, prefix + \".test.txt\")\n",
        "\n",
        "  word_to_id, id_2_word = _build_vocab(train_path)\n",
        "  train_data = _file_to_word_ids(train_path, word_to_id)\n",
        "  valid_data = _file_to_word_ids(valid_path, word_to_id)\n",
        "  test_data = _file_to_word_ids(test_path, word_to_id)\n",
        "  return train_data, valid_data, test_data, word_to_id, id_2_word\n",
        "\n",
        "# Creates an iterator over PTB data\n",
        "# This is like creating a DataLoader with Pytorch, but I used this because more\n",
        "# convenient from a data-managing point of view (easier manipulation of data contained in the dataset)\n",
        "def ptb_iterator(raw_data, batch_size, num_steps):\n",
        "  \"\"\"Iterate on the raw PTB data.\n",
        "  This generates batch_size pointers into the raw PTB data, and allows\n",
        "  minibatch iteration along these pointers.\n",
        "  Args:\n",
        "    raw_data: one of the raw data outputs from ptb_raw_data.\n",
        "    batch_size: int, the batch size.\n",
        "    num_steps: int, the number of unrolls.\n",
        "  Yields:\n",
        "    Pairs of the batched data, each a matrix of shape [batch_size, num_steps].\n",
        "    The second element of the tuple is the same data time-shifted to the\n",
        "    right by one.\n",
        "  Raises:\n",
        "    ValueError: if batch_size or num_steps are too high.\n",
        "  \"\"\"\n",
        "  raw_data = np.array(raw_data, dtype=np.int32)\n",
        "\n",
        "  data_len = len(raw_data)\n",
        "  batch_len = data_len // batch_size\n",
        "  data = np.zeros([batch_size, batch_len], dtype=np.int32)\n",
        "  for i in range(batch_size):\n",
        "    data[i] = raw_data[batch_len * i:batch_len * (i + 1)]\n",
        "\n",
        "\n",
        "  epoch_size = (batch_len - 1) // num_steps\n",
        "\n",
        "  if epoch_size == 0:\n",
        "    raise ValueError(\"epoch_size == 0, decrease batch_size or num_steps\")\n",
        "\n",
        "  for i in range(epoch_size):\n",
        "    x = data[:, i*num_steps:(i+1)*num_steps]\n",
        "    y = data[:, i*num_steps+1:(i+1)*num_steps+1]\n",
        "    yield (x, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAGgY2Y5bmCt"
      },
      "source": [
        "# Get raw datas\n",
        "raw_data = ptb_raw_data(\"data/\")\n",
        "# Get sets and vocabs\n",
        "train_data, valid_data, test_data, vocab, word_to_id = raw_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74Un3FVjdDQd"
      },
      "source": [
        "# **Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDiXg_WVc2xV"
      },
      "source": [
        "import torch.nn as nn\n",
        "# Class for the whole Model\n",
        "class PTBLstm(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, emb_size, hidden_size, num_layers, drop_prob):\n",
        "        super(PTBLstm, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.emb_size = emb_size\n",
        "        # Embedding\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
        "        # LSTM layer\n",
        "        self.lstm = nn.LSTM(input_size=emb_size, hidden_size=hidden_size, num_layers=num_layers, dropout=drop_prob, batch_first = True)\n",
        "        # Dropout layer\n",
        "        self.drp = nn.Dropout(drop_prob)\n",
        "        # Output layer\n",
        "        self.l_out = nn.Linear(in_features=hidden_size, out_features=vocab_size)\n",
        "        self.init_weights()\n",
        "        \n",
        "    def forward(self, inputs, hidden):\n",
        "        # Embedding\n",
        "        x = self.embedding(inputs)\n",
        "        # RNN returns output and last hidden state\n",
        "        outpts, (h, c) = self.lstm(x, hidden)\n",
        "        # Flatten output for feed-forward layer\n",
        "        outpts = outpts.reshape(-1, self.lstm.hidden_size)\n",
        "        # Output layer\n",
        "        outpts = self.l_out(self.drp(outpts))\n",
        "        return outpts, hidden\n",
        "    # Initialize the hidden state in each epoch\n",
        "    def init_state(self, batch_size, device):\n",
        "        weight = next(self.parameters()).data\n",
        "        hidden = (weight.new(self.num_layers, batch_size, self.hidden_size).zero_().to(device), weight.new(self.num_layers, batch_size, self.hidden_size).zero_().to(device))\n",
        "        return hidden\n",
        "    # Initialize the weights of the network when creating it\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        nn.init.uniform_(self.embedding.weight, -initrange, initrange)\n",
        "        nn.init.zeros_(self.l_out.weight)\n",
        "        nn.init.uniform_(self.l_out.weight, -initrange, initrange)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-4kx6Eweesv"
      },
      "source": [
        "# Function that repackages the hidden state.\n",
        "# This is crucial in order to detach the hidden state\n",
        "# from previous gradient history so that torch doesn't have\n",
        "# to check the gradients of the whole dataset each time.\n",
        "def repackage_hidden(h):\n",
        "    if isinstance(h, torch.Tensor):\n",
        "        return h.detach()\n",
        "    else:\n",
        "        return tuple(repackage_hidden(v) for v in h)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8igJtN4dQOk"
      },
      "source": [
        "# **Routines**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDwDDGedf8Di"
      },
      "source": [
        "# Training function\n",
        "def train(net, train_data, batch_size, num_steps, criterion, device):\n",
        "  # Total loss of the epoch\n",
        "  epoch_training_loss = 0\n",
        "  # Total steps of the training (corresponds to the number of batches)\n",
        "  train_steps = 0\n",
        "  # Network in train-mode\n",
        "  net.train()\n",
        "  # Optimizer (Adam with weight decay)\n",
        "  optimizer = torch.optim.AdamW(net.parameters(), lr=lr)\n",
        "  # Initialize the hidden state\n",
        "  hidden = net.init_state(batch_size, device)\n",
        "  for x, y in ptb_iterator(train_data, batch_size, num_steps):\n",
        "        # Repackage in order to detach it from previous grad history\n",
        "        # Convert input to tensor\n",
        "        x = torch.Tensor(x)\n",
        "        # Convert target to tensor\n",
        "        y = torch.LongTensor(y)\n",
        "        # Transfer tensors to device\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        # Zero grad\n",
        "        net.zero_grad()\n",
        "        # Repackaging\n",
        "        hidden = repackage_hidden(hidden)\n",
        "        # Forward pass\n",
        "        outputs, hidden = net.forward(x.long(), hidden)\n",
        "        # Compute loss\n",
        "        loss = criterion(outputs, y.view(-1))\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        # Normalize\n",
        "        torch.nn.utils.clip_grad_norm_(net.parameters(), 0.25)\n",
        "        # Optimization step\n",
        "        optimizer.step()\n",
        "        # Update loss\n",
        "        epoch_training_loss += loss.item()\n",
        "        # # Update steps counter\n",
        "        train_steps += 1\n",
        "  print(\"Training Loss: {}\\t Training Perplexity: {}\".format(epoch_training_loss/train_steps, np.exp(epoch_training_loss/train_steps)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8paYDFDiQqC"
      },
      "source": [
        "# Function for evaluation\n",
        "def evaluate(net, valid_data, batch_size, num_steps, criterion, device):\n",
        "    # Total loss of the epoch\n",
        "    epoch_validation_loss = 0\n",
        "    # Total steps of the validation (corresponds to the number of batches)\n",
        "    val_steps = 0\n",
        "\n",
        "    hidden = net.init_state(batch_size, device)\n",
        "    # Network evaluation mode\n",
        "    net.eval()\n",
        "    \n",
        "    for x, y in ptb_iterator(valid_data, batch_size, num_steps):\n",
        "        # Repackage in order to detach it from previous grad history\n",
        "        hidden = repackage_hidden(hidden)\n",
        "        # Convert input to tensor\n",
        "        x = torch.Tensor(x)\n",
        "        # Convert target to tensor\n",
        "        y = torch.LongTensor(y)\n",
        "        # Tensors to device\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        # Forward pass\n",
        "        net.zero_grad()\n",
        "        outputs, hidden = net.forward(x.long(), hidden)\n",
        "        # Compute loss\n",
        "        loss = criterion(outputs, y.view(-1))\n",
        "        # Update loss\n",
        "        epoch_validation_loss += loss.item()\n",
        "        # Update steps counter\n",
        "        val_steps += 1\n",
        "    return epoch_validation_loss/val_steps"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NzLJxG1RK5G"
      },
      "source": [
        "# **Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3ELa4oRdPkX",
        "outputId": "94b8c8ed-a75e-4555-ede2-6eb87a60201b"
      },
      "source": [
        "import torch\n",
        "# Define Hyperparameters\n",
        "batch_size = 64\n",
        "# Number of unfoldings\n",
        "num_steps = 20\n",
        "epochs = 10\n",
        "# Embedding vector size\n",
        "emb_size = 650\n",
        "hidden_size = 650\n",
        "num_layers = 2\n",
        "# Dropout probability\n",
        "drop_prob = 0.5\n",
        "# Learning rate\n",
        "lr = 0.001\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device used:\", device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device used: cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TiRSYmLFdhF0",
        "outputId": "0db6a5ec-0127-4d75-d69b-e0d4f10b1ae6"
      },
      "source": [
        "# Creation of the network\n",
        "net = PTBLstm(len(vocab), emb_size, hidden_size, num_layers, drop_prob)\n",
        "net.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PTBLstm(\n",
              "  (embedding): Embedding(10000, 650)\n",
              "  (lstm): LSTM(650, 650, num_layers=2, batch_first=True, dropout=0.5)\n",
              "  (drp): Dropout(p=0.5, inplace=False)\n",
              "  (l_out): Linear(in_features=650, out_features=10000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MU5OoCed6fb"
      },
      "source": [
        "# Criterion for the loss\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hcel2aUGeDu8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "outputId": "b651fe8e-dd6f-4f3a-a512-114909894701"
      },
      "source": [
        "# Training\n",
        "for i in range(epochs):\n",
        "  train(net, train_data, batch_size, num_steps, criterion, device)\n",
        "  loss = evaluate(net, valid_data, batch_size, num_steps, criterion, device)\n",
        "  print(\"Epoch: {}\\tValidation Loss: {}\\tValidation Perplexity: {}\".format(i, loss, np.exp(loss)))\n",
        "# Save the model and download it on your computer\n",
        "torch.save(net.state_dict(), 'model.obj')\n",
        "from google.colab import files\n",
        "files.download('model.obj') \n",
        "print(\"おわりです :)\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Loss: 5.947496254582051\t Training Perplexity: 382.7937202187547\n",
            "Epoch: 0\tValidation Loss: 5.419042729494865\tValidation Perplexity: 225.6629985413868\n",
            "Training Loss: 5.386480979682985\t Training Perplexity: 218.4333600489644\n",
            "Epoch: 1\tValidation Loss: 5.1982702706989485\tValidation Perplexity: 180.9589609905856\n",
            "Training Loss: 5.182423074054981\t Training Perplexity: 178.11387163772352\n",
            "Epoch: 2\tValidation Loss: 5.082651849378619\tValidation Perplexity: 161.20097031506222\n",
            "Training Loss: 5.046653003403635\t Training Perplexity: 155.50113076657712\n",
            "Epoch: 3\tValidation Loss: 5.007829657772131\tValidation Perplexity: 149.57974436928626\n",
            "Training Loss: 4.94117413013763\t Training Perplexity: 139.93445440241834\n",
            "Epoch: 4\tValidation Loss: 4.954683370757521\tValidation Perplexity: 141.8376893021859\n",
            "Training Loss: 4.854567114643486\t Training Perplexity: 128.3251291203103\n",
            "Epoch: 5\tValidation Loss: 4.916038931461802\tValidation Perplexity: 136.461009817906\n",
            "Training Loss: 4.778748434108808\t Training Perplexity: 118.95537634745054\n",
            "Epoch: 6\tValidation Loss: 4.887562007234807\tValidation Perplexity: 132.62982900403728\n",
            "Training Loss: 4.712186249162868\t Training Perplexity: 111.29521318803539\n",
            "Epoch: 7\tValidation Loss: 4.876678441700182\tValidation Perplexity: 131.19417028247557\n",
            "Training Loss: 4.6530832515275184\t Training Perplexity: 104.90794502039924\n",
            "Epoch: 8\tValidation Loss: 4.862284375910173\tValidation Perplexity: 129.31927879121736\n",
            "Training Loss: 4.597586600248479\t Training Perplexity: 99.244509709468\n",
            "Epoch: 9\tValidation Loss: 4.856237193994355\tValidation Perplexity: 128.53962132830094\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_45b034b3-669a-4092-8aee-ff64d205588e\", \"model.obj\", 79124179)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "おわりです :)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPG47cVTnsGl"
      },
      "source": [
        "# **Testing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jE3JL1Onrgs",
        "outputId": "1b9dd541-9464-481c-9932-337c8da366d1"
      },
      "source": [
        "# Test the network\n",
        "loss = evaluate(net, test_data, batch_size, num_steps, criterion, device)\n",
        "print(f'Testing loss: {loss}, Testing perplex: {np.exp(loss)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing loss: 4.795990131795406, Testing perplex: 121.0241523401952\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
